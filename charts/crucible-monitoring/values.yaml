# Crucible Monitoring Chart Values
# This chart deploys the monitoring and observability stack for Crucible

# IMPORTANT: This chart assumes crucible-infra is already deployed
# The infra chart provides: Ingress Controller, CA certificates

global:
  # Domain name for the Crucible deployment
  domain: ""

  # Kubernetes namespace
  namespace: default

  # TLS secret name (accessible to subcharts via global values)
  tlsSecretName: ""

# Custom CA Certificates Configuration
# Enable this if Grafana needs to trust custom CA certificates (e.g., for OAuth with Keycloak behind corporate proxy)
# When enabled, ALL certificate files in the ConfigMap will be mounted to Grafana
caCerts:
  enabled: false
  configMapName: "" # Name of ConfigMap containing CA certificates

# Prometheus - Metrics Collection and Storage
# https://artifacthub.io/packages/helm/prometheus-community/prometheus
# https://prometheus.io/docs/introduction/overview/
prometheus:
  enabled: true
  server:
    # Enable remote-write receiver for Alloy to send metrics
    extraArgs:
      web.enable-remote-write-receiver: ""

# Grafana - Dashboards and Visualization
# https://artifacthub.io/packages/helm/grafana/grafana
# https://grafana.com/docs/grafana/latest/
#
# TODO: Create and import custom dashboards: https://github.com/grafana/helm-charts/tree/main/charts/grafana#import-dashboards
grafana:
  enabled: true

  # Ingress configuration
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      nginx.ingress.kubernetes.io/configuration-snippet: |
        rewrite ^(/grafana)$ $1/ break;
    hosts:
      - "{{ .Values.global.domain }}"
    path: /grafana
    pathType: Prefix
    tls:
      - secretName: "{{ .Values.global.tlsSecretName }}"
        hosts:
          - "{{ .Values.global.domain }}"

  # Grafana configuration
  grafana.ini:
    server:
      root_url: "https://{{ .Values.global.domain }}/grafana/"
      serve_from_sub_path: true

    auth:
      disable_login_form: true # OAuth only (no local users)
      signout_redirect_url: ""

    # OAuth integration
    auth.generic_oauth:
      enabled: false
      name: ""
      allow_sign_up: true
      client_id: grafana
      client_secret: ${GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET}
      scopes: "openid profile email"
      auth_url: ""
      token_url: ""
      api_url: ""
      email_attribute_path: email
      login_attribute_path: email
      name_attribute_path: preferred_username
      # This allows Keycloak users in the Admin or Viewer groups to use grafana
      role_attribute_path: "contains(realm_access.roles[*], 'Administrator') && 'Admin' || 'Viewer'"

  # Configure datasources for Prometheus, Loki, and Tempo
  datasources:
    datasources.yaml:
      apiVersion: 1
      # datasources use k8s service addresses to keep traffic cluster-internal
      datasources:
        - name: Prometheus
          uid: prometheus
          type: prometheus
          url: "http://{{ .Release.Name }}-prometheus-server"
          isDefault: true
        - name: Loki
          uid: loki
          type: loki
          url: "http://{{ .Release.Name }}-loki:3100"
          jsonData:
            maxLines: 1000
        - name: Tempo
          uid: tempo
          type: tempo
          url: "http://{{ .Release.Name }}-tempo:3100"
          jsonData:
            httpMethod: GET
            tracesToMetrics:
              datasourceUid: prometheus
            serviceMap:
              datasourceUid: prometheus

  # Env var can optionally be used for OAuth client secret for integration
  env:
    GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: ""

  # NOTE: To enable custom CA certificates for Grafana, set caCerts.enabled: true
  # Use the following configuration when deploying with custom certs:
  # env:
  #   SSL_CERT_FILE: /etc/ssl/certs/combined-ca/ca-certificates.crt
  # extraInitContainers:
  #   - name: grafana-ca-bundle
  #     image: alpine:3.19
  #     command:
  #       - /bin/sh
  #       - -c
  #       - |
  #         set -e
  #         cp /etc/ssl/certs/ca-certificates.crt /ca-bundle/ca-certificates.crt
  #         # Append all certificate files from the ConfigMap
  #         for cert in /crucible-ca/*; do
  #           if [ -f "$cert" ]; then
  #             cat "$cert" >> /ca-bundle/ca-certificates.crt
  #           fi
  #         done
  #     volumeMounts:
  #       - name: ca-bundle
  #         mountPath: /ca-bundle
  #       - name: crucible-ca
  #         mountPath: /crucible-ca
  # extraVolumes:
  #   - name: crucible-ca
  #     configMap:
  #       name: "{{ .Values.caCerts.configMapName }}"
  #       # No items specified - mounts ALL keys from ConfigMap
  #   - name: ca-bundle
  #     emptyDir: {}
  # extraVolumeMounts:
  #   - name: ca-bundle
  #     mountPath: /etc/ssl/certs/combined-ca
  #     readOnly: true

# Loki - Log Aggregation
# https://artifacthub.io/packages/helm/grafana/loki
# https://grafana.com/docs/loki/latest/
loki:
  enabled: true

  # SingleBinary mode for simplified deployment
  # https://grafana.com/docs/loki/latest/get-started/deployment-modes/
  deploymentMode: SingleBinary

  # Disable gateway (not needed for small deployments)
  gateway:
    enabled: false

  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      size: 5Gi

  # Disable read/write/backend for SingleBinary mode
  read:
    replicas: 0
  write:
    replicas: 0
  backend:
    replicas: 0

  # Loki configuration
  loki:
    # Disable auth for simplified deployment (cluster-internal only)
    auth_enabled: false

    # Use filesystem storage
    storage:
      type: filesystem
      filesystem:
        chunks_directory: /var/loki/chunks
        rules_directory: /var/loki/rules

    commonConfig:
      replication_factor: 1

    # Use test schema for simplified deployment
    useTestSchema: true

# Tempo - Distributed Tracing
# https://artifacthub.io/packages/helm/grafana/tempo
# https://grafana.com/docs/tempo/latest/
tempo:
  enabled: true
  replicas: 1

  # Tempo configuration
  tempo:
    # Use local storage for trace data
    storage:
      trace:
        backend: local
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal

    # OTLP receivers for gRPC and HTTP
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"

  # Persistent storage for traces
  persistence:
    enabled: true
    size: 5Gi

  # Disable Tempo Query (use Grafana instead)
  tempoQuery:
    enabled: false

# Grafana Alloy - OpenTelemetry Collector
# Routes logs to Loki, metrics to Prometheus, traces to Tempo
# https://artifacthub.io/packages/helm/grafana/alloy
# https://grafana.com/docs/alloy/latest/
alloy:
  enabled: true

  controller:
    type: daemonset

  alloy:
    stabilityLevel: experimental

    # Mount log directories for Kubernetes log collection
    mounts:
      varlog: true
      dockercontainers: true

    # Extra ports for OTLP receivers
    extraPorts:
      - name: otlp-grpc
        port: 4317
        targetPort: 4317
        protocol: TCP
      - name: otlp-http
        port: 4318
        targetPort: 4318
        protocol: TCP

    configMap:
      # Grafana Alloy Configuration
      # The default configuration will:
      # - Discover and tail logs from all Kubernetes pods in the cluster and forward to Loki
      # - Receive OpenTelemetry metrics, logs, and traces from instrumented applications
      # - Export OTLP metrics to Prometheus via remote write
      # - Export OTLP logs to Loki
      # - Export OTLP traces to Tempo via gRPC
      # - Batch OTLP exports to reduce network overhead
      #
      # Reference:
      #    - https://grafana.com/docs/alloy/latest/reference/components/
      #    - https://grafana.com/docs/alloy/latest/collect/logs-in-kubernetes/
      #    - https://grafana.com/docs/alloy/latest/collect/opentelemetry-to-lgtm-stack/
      #
      # TODO: Consider adding the following:
      # 1. Kubernetes Cluster Events: https://grafana.com/docs/alloy/latest/collect/logs-in-kubernetes/#kubernetes-cluster-events
      # 2. Kuberenetes Node Logs: https://grafana.com/docs/alloy/latest/collect/logs-in-kubernetes/#system-logs
      # 3. Grafana Alloy Meta-Metrics (metrics about the metrics collector): https://grafana.com/docs/alloy/latest/collect/metamonitoring/
      content: |-
        logging {
          level = "info"
        }

        // Begin Kubernetes discovery configuration

        // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
        // this spec will automatically discover all kubernetes pods in the cluster
        discovery.kubernetes "pods" {
          role = "pod"
        }

        // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
        // this simplifies the names of labels
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pods.targets

          // Label creation - "namespace" field from "__meta_kubernetes_namespace"
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action        = "replace"
            target_label  = "namespace"
          }

          // Label creation - "pod" field from "__meta_kubernetes_pod_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action        = "replace"
            target_label  = "pod"
          }

          // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "container"
          }

          // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action        = "replace"
            target_label  = "app"
          }

          // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action        = "replace"
            target_label  = "job"
            separator     = "/"
            replacement   = "$1"
          }
        }

        // End Kubernetes discovery configuration

        // Begin Loki configuration

        // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
        loki.source.kubernetes "pods" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pods.receiver]
        }

        // loki.process receives log entries from other Loki components, applies one or more processing stages,
        // and forwards the results to the list of receivers in the component's arguments.
        loki.process "pods" {
          stage.docker {}
          forward_to = [loki.write.default.receiver]
        }

        // loki.write sends logs to the Loki endpoint
        loki.write "default" {
          endpoint {
            url = "http://{{ .Release.Name }}-loki:3100/loki/api/v1/push"
          }
        }

        // End Loki configuration

        // Begin Prometheus configuration

        // Remote write endpoint for OTLP metrics
        // Application metrics received via OTLP will be forwarded here
        prometheus.remote_write "default" {
          endpoint {
            url = "http://{{ .Release.Name }}-prometheus-server/api/v1/write"
          }
        }

        // NOTE: Pod scraping is disabled in Alloy.
        // Prometheus handles infrastructure metric scraping directly using its own
        // Kubernetes service discovery (node-exporter, kube-state-metrics, etc.).
        // Alloy focuses on receiving OTLP telemetry from instrumented applications.

        // End Prometheus configuration

        // Begin OpenTelemetry configuration

        // Export otel logs to Loki
        otelcol.exporter.loki "default" {
          forward_to = [loki.write.default.receiver]
        }

        // Export otel traces to Tempo via gRPC
        otelcol.exporter.otlp "default" {
          client {
            endpoint = "{{ .Release.Name }}-tempo:4317"
            // insecure is ok since this just stays in the cluster
            tls {
              insecure = true
            }
          }
        }

        // Export otel metrics to Prometheus
        otelcol.exporter.prometheus "default" {
          forward_to = [prometheus.remote_write.default.receiver]
        }

        // Batch otel exports to reduce network chatter
        otelcol.processor.batch "default" {
          output {
            metrics = [otelcol.exporter.prometheus.default.input]
            logs    = [otelcol.exporter.loki.default.input]
            traces  = [otelcol.exporter.otlp.default.input]
          }
        }

        // Configure an otel receiver for apps to send otel to
        // This is a compatible Otel Collector for http and grpc
        otelcol.receiver.otlp "default" {
          grpc {
            endpoint = "0.0.0.0:4317"
          }

          http {
            endpoint = "0.0.0.0:4318"
          }

          // Send output to the batch processor
          // Batch processor then forwards to the correct services
          output {
            metrics = [otelcol.processor.batch.default.input]
            logs    = [otelcol.processor.batch.default.input]
            traces  = [otelcol.processor.batch.default.input]
          }
        }

        // End OpenTelemetry configuration
